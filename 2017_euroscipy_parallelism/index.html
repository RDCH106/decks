<!DOCTYPE html>
<html>
<head>
<title>Loky</title>
<meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="./remark/slides.css">
<link rel="stylesheet" href="./mermaid/mermaid.forest.min.css">
<link rel="stylesheet" href="./font-awesome-4.7.0/css/font-awesome.min.css">

</head>
  <body>
    <textarea id="source">
class: center, middle

# Tricks for Efficient Multicore Computing

.normal[
<br/>
Thomas Moreau - **Olivier Grisel** 
<br/>
]

---
# Outline

### Introduction to the `concurrent.futures` API

### Thread-based vs Process-based multicore computing

### Thread-based parallelism for array processing with BLAS

---
# Embarassingly parallel computation

.normal2[
Three APIs available:
- `multiprocessing`: first implementation.<br/><br/>

- `concurrent.futures`: reimplementation using `multiprocessing` under the hood.<br/><br/>

- `loky`: robustification of `concurrent.futures`.<br/><br/>
]


---
## The `Future` object: an asynchronous result state.

### States

.normal[

`Future`: reference to the result of some asynchronous computation

4 states:
.state[Not started], .state[Running], .state[Cancelled] and .state[Done]

The state of a `Future` can be checked using `f.running, f.cancelled, f.done`.

]


.left-column[.normal[
### Blocking methods

* `f.result(timeout=None)`
* `f.exception(timeout=None)`
]]
.right-column[.normal[
<br/>
<br/>
<br/>
wait for computations to be done.
]]


---
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

*def fit_model(params):
*   # Heavy computation
*   return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]


.small-right-column[
![:scale 100%](images/1_init.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

*# Create an executor with 4 threads
*with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/2_spawn.png)

]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

*   # Submit an asynchronous job and return a Future
*   future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/3_submit1.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

*   # Submit other job
*   future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/4_submit2.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

*   # Run other computation
*   ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/5_running.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

*   # Blocking call, wait and return the result
*   model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/6_collect1.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

*   # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
*   model2 = future2.result(timeout=None)

# The ressources have been cleaned up
print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/7_collect2.png)
]

---
count: false
### The `Executor`: a worker pool

.wide-left-column[
```python
from concurrent.futures import ThreadPoolExecutor

def fit_model(params):
    # Heavy computation
    return model

# Create an executor with 4 threads
with ThreadPoolExecutor(max_workers=4) as executor:

    # Submit an asynchronous job and return a Future
    future1 = executor.submit(fit_model, param1)

    # Submit other job
    future2 = executor.submit(fit_model, param2)

    # Run other computation
    ...

    # Blocking call, wait and return the result
    model1 = future1.result(timeout=None)
    model2 = future2.result(timeout=None)

*# The ressources have been cleaned up
*print(model1, model2)
```
]

.small-right-column[
![:scale 100%](images/8_final.png)
]

<!---


<div class="mermaid">
classDiagram
    MainProcess .. CommunicationQueue
    CommunicationQueue .. Worker1
    CommunicationQueue .. Worker2
    CommunicationQueue .. Worker3
    CommunicationQueue .. Worker4
    MainProcess : fit_model()
</div>
-->



---
class: middle, center

# Choosing the type of worker:

# `Thread` or `Process` ?

---
# Thread Worker
.left-column[.normal[

- Real system thread:
    - pthread 
    - windows thread

- All the computation are done with a **single** interpreter.

]]
<div class="right-column" style="margin: -50px;">
.normal[
**Advantages:**

- Fast spawning

- Low memory overhead

- No communication overhead (shared python objects)
]</div>
.reset-column[
]

--
count: false

.normal[.centered[
    <br/>
    <br/>
Wait... shared python objects and a single interpreter?!?

]]

--
count: false

.centered[
    ## There is only one GIL!
]

---
# Python Global Interpreter Lock (GIL)

.normal[

- Only one thread can acquire it at a time.

- Not designed for efficient multicore computing.

**Global lock everytime we access a python object.**

Released when performing long I/O operations

Released by some libraries: numpy, pandas, sklearn, Cython `with nogil`...
]

---
# Thread Worker

.normal[
Multiple threads running python code:
![:scale 85%](images/thread-2.png)

This is not quicker than sequential even on a multicore machine.
]

---
# Thread Worker

.normal[
Threads hold the GIL when running python code.  
They release it when blocking for I/O:
![](images/thread-0.png)  
Or when calling into some native library:
![](images/thread-1.png)

]


---
exclude: true
# Thread Worker

.left-column[
Even worse:

```python
>>> from threading import Thread
>>> def count(n):
...     while n > 0:
...         n -= 1

>>> %timeit count(10000000)
1 loop, best of 3: 409 ms per loop

>>> %%timeit
... t1 = Thread(target=count,args=(10000000,))
... t2 = Thread(target=count,args=(10000000,))
... t1.start(); t2.start()
... t1.join(); t2.join()

1 loop, best of 3: 836 ms per loop
```
]

.right-column[.normal[
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    The threads spend more time trying to acquire the <b>GIL</b> than computing.
]]


---
# Process Worker
.left-column[.normal[

- Create a new python interpreter per worker.

- Each worker run in **its own ** interpreter.

]]

<div class="right-column" style="margin-top: -50px;">
.normal[
**Inconvenients:**
- *Slow* spawning

- Higher memory overhead

- Higher communication overhead.
]</div>
.reset-column[
]

--
count: false

.normal[
**But there is GIL contention anymore!**
<br/></br/>
The computation can be done in parallel even for python code.

]

--
count: false

.normal[
<br/>
Methods to create a new interpreter: *fork* or *spawn*

]

---
# Launching a new interpreter: *fork*


.normal[

Duplicate the current interpreter. (Only available on UNIX)


.left-column[
**Advantages:**

- Low spawning overhead.

- The interpreter is warm *imported*.

]
.right-column[
**Inconvenient:**

- Bad interaction with multithreaded programs

- Does not respect the POSIX specifications

]
.reset-column[

]
$\Rightarrow$ Some libraries crash: numpy on OSX (Accelerate), OpenMP from GCC...
]


---
# Launching a new interpreter: *spawn*


.normal[

Create a new interpreter from scratch.


.left-column[
**Advantages:**

- Safe (respect POSIX)

- Fresh interpreter without extra libraries.

]
.right-column[
**Inconvenient:**

- Slower to start  (50-300ms)

- Need to reload the libraries, redefine the functions...

]
]

---
# `Thread` vs `Process`

.table[
|           |  Thread   | Process .subitem[(fork)] | Process .subitem[(spawn)] |
|---------------|:------------------:|:------------------:|:------------------:|
| Efficient multicore on pure Python code | ![](images/no.jpg) | ![](images/ok.jpg) | ![](images/ok.jpg) |
| No communication overhead | ![](images/ok.jpg) | ![](images/no.jpg) | ![](images/no.jpg) |
| POSIX safe    | ![](images/ok.jpg) | ![](images/no.jpg) | ![](images/ok.jpg) |
| No spawning overhead | ![](images/ok.jpg) | ![](images/ok.jpg) | ![](images/no.jpg) |
]

---
# `Thread` vs `Process`

.table[
|           |  Thread   | Process .subitem[(fork)] | Process .subitem[(spawn)] |
|---------------|:------------------:|:------------------:|:------------------:|
| Efficient multicore on pure Python code | ![](images/no.jpg) | ![](images/ok.jpg) | ![](images/ok.jpg) |
| No communication overhead | ![](images/ok.jpg) | ![](images/no.jpg) | ![](images/no.jpg) |
| POSIX safe    | ![](images/ok.jpg) | ![](images/no.jpg) | ![](images/ok.jpg) |
| No spawning overhead | ![](images/ok.jpg) | ![](images/ok.jpg) | <img src="images/no_loky.png" alt="" style="width: 4em;"> |
]

.centered[.normal[
<br/>
$\Rightarrow$ Hide the spawning overhead by reusing the pool of processes.
]]

---
class: middle, center

# Reusing a `ProcessPoolExecutor`.

---
# Reusing a `ProcessPoolExecutor`.

<br/>
.normal2[

To Avoid the spawning overhead, reuse a previously started `ProcessPoolExecutor`.

The spawning overhead is only paid once.

Easy using a global pool of process.  
__Main issue:__ is that robust?
]

---
# Managing the state of the executor

.normal[
Example deadlock:
```python
>>> from concurrent.futures import ProcessPoolExecutor
>>> with ProcessPoolExecutor(max_workers=4) as e:
*...     e.submit(lambda: 1).result()
...     
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/queues.py", line 241, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0x7fcff0184d08>:
attribute lookup <lambda> on __main__ failed

^C

```

It can be tricky to know which `submit` call crashed the `Executor`.
]


---
# Managing the state of the executor

.normal[
Example deadlock:
```python
>>> from concurrent.futures import ProcessPoolExecutor
>>> with ProcessPoolExecutor(max_workers=4) as e:
*...     e.submit(lambda: 1)
...
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/queues.py", line 241, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <function <lambda> at 0x7f5c787bd488>:
attribute lookup <lambda> on __main__ failed

^C

```

Even worse, shutdown itself is deadlocked.

]

---
exclude: true
# Managing the state of the executor

.normal[
Example deadlock n2:
```python
>>> from concurrent.futures import ProcessPoolExecutor
>>> def error():
...     raise RuntimeError()
... 
>>> class CrashAtUnpickle(object):
...     """Bad object that triggers a segfault at unpickling time."""
...     def __reduce__(self):
...         return error, ()
...     
>>> def return_instance(cls):
...     return cls()
... 
>>> with ProcessPoolExecutor(max_workers=4) as e:
...     e.submit(return_instance, CrashAtUnpickle).result()
...     
Exception in thread Thread-132:
Traceback (most recent call last):
    ....
    raise RuntimeError()
RuntimeError

^C
```
]

---
class: middle, center

# Reusable pool of workers: `loky`.

---
# `loky`: a robust pool of workers


.normal[
```python
>>> from loky import ProcessPoolExecutor
>>> class CrashAtPickle(object):
...     """Bad object that triggers a segfault at unpickling time."""
...     def __reduce__(self):
...         raise RuntimeError()
...     
>>> with ProcessPoolExecutor(max_workers=4) as e:
...     e.submit(CrashAtPickle()).result()
...     
Traceback (most recent call last):
...
RuntimeError
Traceback (most recent call last):
...
BrokenExecutor: The QueueFeederThread was terminated abruptly while feeding a
new job. This can be due to a job pickling error.
>>>
```

- Return and raise a user friendly exception.
- Fix some other deadlocks.


]

---
# A reusable `ProcessPoolExecutor`

.left-column[
```python
*>>> from loky import get_reusable_executor
*>>> excutor = get_reusable_executor(max_workers=4)
*>>> print(excutor.executor_id)
*0

>>> excutor.submit(id, 42).result()
139655595838272

>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

>>> excutor.submit(CrashAtUnpickle()).result()
Traceback (most recent call last):
...
BrokenExecutorError
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
1
>>> excutor.submit(id, 42).result()
139655595838272

```
]

.right-column[

Create a `ProcessPoolExecutor` using the factory function `get_reusable_executor`.

.small[]
]


---
count: false
# A reusable `ProcessPoolExecutor`

.left-column[
```python
>>> from loky import get_reusable_executor
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

*>>> excutor.submit(id, 42).result()
*139655595838272

>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

>>> excutor.submit(CrashAtUnpickle()).result()
Traceback (most recent call last):
...
BrokenExecutorError
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
1
>>> excutor.submit(id, 42).result()
139655595838272

```
]
.right-column[
Create a `ProcessPoolExecutor` using the factory function `get_reusable_executor`.

The executor can be used exactly as `ProcessPoolExecutor`.
]


---
count: false
# A reusable `ProcessPoolExecutor`

.left-column[
```python
>>> from loky import get_reusable_executor
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

>>> excutor.submit(id, 42).result()
139655595838272

*>>> excutor = get_reusable_executor(max_workers=4)
*>>> print(excutor.executor_id)
*0

>>> excutor.submit(CrashAtUnpickle()).result()
Traceback (most recent call last):
...
BrokenExecutorError
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
1
>>> excutor.submit(id, 42).result()
139655595838272

```
]
.right-column[
Create a `ProcessPoolExecutor` using the factory function `get_reusable_executor`.

The executor can be used exactly as `ProcessPoolExecutor`.

When the factory is called elsewhere, reuse the same executor if it is working.
]


---
count: false
# A reusable `ProcessPoolExecutor`

.left-column[
```python
>>> from loky import get_reusable_executor
>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

>>> excutor.submit(id, 42).result()
139655595838272

>>> excutor = get_reusable_executor(max_workers=4)
>>> print(excutor.executor_id)
0

>>> excutor.submit(CrashAtUnpickle()).result()
Traceback (most recent call last):
...
*BrokenExecutorError
*>>> excutor = get_reusable_executor(max_workers=4)
*>>> print(excutor.executor_id)
*1
>>> excutor.submit(id, 42).result()
139655595838272

```
]
.right-column[
Create a `ProcessPoolExecutor` using the factory function `get_reusable_executor`.

The executor can be used exactly as `ProcessPoolExecutor`.

When the factory is called elsewhere, reuse the same executor if it is working.

When the executor is broken, automatically re-spawn a new one.
]


---
class: middle, center

# Thread-based multicore computing for arrays with BLAS

---
# BLAS

### Numpy is a Python wrapper for BLAS routines

### Efficient implementations:

- OpenBLAS (numpy from PyPI or conda-forge)
- Intel MKL (numpy from Anaconda)

### `np.dot(A, B)` will use all cores by default

### Control number of threads with env variables:

- `OPENBLAS_NUM_THREADS=2 python mycode.py`

- `MKL_NUM_THREADS=2 python mycode.py`

---
class: center, middle

# Roofline Analysis

---
class: center, middle

![MKL roofline](images/roofline_mkl.png)

---
class: center, middle

![OpenBLAS roofline](images/roofline_openblas.png)

---
class: center, middle

![MKL roofline 10 cores](images/roofline_mkl_10_cores.png)

---
class: center, middle

![Speed up 10 cores](images/speedup_mkl_10_cores.png)

---
class: middle, center

## Multicore scalability is limited by I/O for workloads with low arithmetic intensity


---
class: middle, center

# Conclusion

---
# Conclusion

.normal3[

- `Thread` is the best method to run multicore programs if your code releases the **GIL** in performance critical sections.

- Array operations already benefit from threading via numpy + BLAS (e.g. MKL or OpenBLAS)

- `loky` uses `Process` with `spawn` and tries to reuse the pool of process as much as possible.

- `loky` will be used by default in future `joblib` and `scikit-learn`.

]

---

class: middle, center

# Thanks for your attention!

.normal[
<br/>
<div style="text-align:left">
Slides available at [ogrisel.github.io/decks/2017_euroscipy_parallelism](https://ogrisel.github.io/decks/2017_euroscipy_parallelism)
<br/><br/>
Based on Thomas Moreau slides available at [tommoral.github.io/pyparis17/](https://tommoral.github.io/pyparis17/)
<br/><br/>
More on the GIL by Dave Beazley : [dabeaz.com/python/GIL.pdf](http://dabeaz.com/python/GIL.pdf)<br/><br/>
![:scale 1em](images/github.png) Loky project : [github.com/tommoral/loky](https://github.com/tommoral/loky)<br/><br/>
![:scale 1em](images/twitter.png) @[ogrisel](https://twitter.com/ogrisel) <br/>
</div>
]
.filler[]



</textarea>
<style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<!--<script type="text/javascript" src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="./remark/remark.min.js" type="text/javascript"> </script>
<script src="./mermaid/mermaid.min.js"></script>
<script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
    var slideshow = remark.create(config_remark);


    // Configure Mermaid diagrams
    /*var config_mermaid = {
        startOnLoad: false,
        flowchart:{
                useMaxWidth:false,
                htmlLabels:true
        },
        gantt:{
            barHeight: 24,
            fontSize: 24
        }
    };
    mermaid.initialize(config_mermaid);

    // Fix the slide display in remark
    function initMermaid(s) {
        var diagrams = document.querySelectorAll('.mermaid');
        var i;
        for(i=0;i<diagrams.length;i++){
            if(diagrams[i].offsetWidth > 0){
                mermaid.init(undefined, diagrams[i]);
            }
        }
    }
    slideshow.on('afterShowSlide', initMermaid);*/

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });

</script>
</body>
</html>
